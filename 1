1111111111111111111111111111111111111                             Practical-1 data cleaning                         111111111111111111111111111111111111111

flight = read.csv("flights.csv")
View(flight)
summary(flight) 
#dimensions
 dim(flight)
#changing name
 names(flight)[21] = 'type of passenger'
 flight
#max fare
 max(flight$total.fare)
#missing values in age
 flight$age[is.na(flight$age)] = round(mean(flight$age, na.rm = TRUE))
 flight$age
#missing values in adult 
flight$'type of passenger'[flight$`type of passenger`%in%c('')] = 'Adult'
flight
#mean fare
mean(flight$total.fare, na.rm = TRUE)
#deleting rows with no verification
flight[flight$verification.details !='',]
#to see international bookings
fc = flight[flight$type.of.booking == 'International',]
fc
#transforming
flight$gender[flight$gender == "Male"] = "M"
flight$gender[flight$gender == "male"] = "M"
flight$gender[flight$gender == "Female"] = "F"
flight$gender[flight$gender == "female"] = "F"
flight
#replacing flight possibility % exceeding 100 with 100
flight$flight.possibility.percentage[flight$flight.possibility.percentage > 100] = 100
flight



22222222222222222222222                         Practical-2 data pre-processing, exploratory analysis and visualization                222222222222222222222222222

#package
library(caret)
#data pre-processing
data("iris")
View(iris)
#dimensions
dim(iris)
#type of each attribute
sapply(iris, class)
head(iris)
summary(iris)
#creating validation set
validation = createDataPartition(iris$Species, p = 0.8, list = FALSE)
validation20 = iris[-validation,]

iris = iris[validation,]
View(iris)
#visualizing the data
x = iris[,1:4]
y = iris[,5]
#univariate plots
boxplot(x[,1], main = names(iris)[1])
#plotting the area
par(mfrow = c(1,4))
for (i in 1:4){boxplot(x[i], main=names(iris)[i])}
#multivariate plots
#scatter
dev.off()
featurePlot(x=x, y=y, plot = "pairs")


333333333333333333333333333                           Practical-3 CLASSIFICATION                                           33333333333333333333333

#3a on iris dataset

data("iris")
View(iris)
summary(iris)
set.seed(555)
#partitioning the data
ind = sample(2, nrow(iris), replace = TRUE, prob = c(0.8, 0.2))
#storing
train = iris[ind ==1,]
train
test = iris[ind ==2,]
test
#packages for decision tree
library(party)
library(rpart)
library(rpart.plot)
#creating decision tree for train data
tree = rpart(Species~.,train)
rpart.plot(tree)
#creating decision tree for test data
tree1 = rpart(Species~., test)
rpart.plot(tree1)

#3b using titanic dataset

library(caret)
titanic = read.csv("Titanic (1).csv")
summary(titanic)
#selecting only 4 columns for analysis
titanic = titanic[,c("PClass","Age","Sex","Survived")]
#using as.factor
titanic$Survived = as.factor(ifelse(titanic$Survived==0,"Died","Survived"))
titanic$PClass = as.factor(titanic$PClass)
titanic$Sex = as.factor(titanic$Sex)
titanic
#datatype
str(titanic)
#cleaning the data
#omitting na's
titanic = na.omit(titanic)
set.seed(9999)
View(titanic)
#partitioning the data
train = createDataPartition(titanic[,"Survived"], p=0.8, list = FALSE)
titanic.trn = titanic[train,]
titanic.tst = titanic[-train,]
#cross validation
ctrl = trainControl(method = "cv", number = 10)
fit.cv = train(Survived~., data= titanic.trn, method = "rpart", trControl = ctrl)
#predicting
pred = predict(fit.cv, titanic.tst)
pred
#creating a confusion matrix
confusionMatrix(table(titanic.tst[, "Survived"],pred))
print(fit.cv)
#plotting
plot(fit.cv)
library(rpart.plot)
rpart.plot(fit.cv$finalModel)



44444444444444444444444444444444444                            #practical-4 Naive BAYESSS                                   44444444444444444444444444444444444
#4a using cars dataset

data = read.csv("cars.csv")
print(data)
#datatype
str(data)
#packages
library(e1071)
library(caTools)
library(caret)
#splitting data
sp = sample.split(data$stolen, SplitRatio = 0.8)
train = subset(data, sp=="TRUE")
train
test = subset(data, sp =="FALSE")
test
set.seed(120)
#fitting model
classifier = naiveBayes(stolen~., data = train)
classifier
#predicting
y_pred = predict(classifier,  newdata = test)
#confusion matrix
cmm = table(test$stolen, y_pred)
cmm

#4b using flights dataset

data1 = read.csv("flights.csv")
str(data1)
#packages
library(e1071)
library(caret)
library(caTools)
#splitting the data
sp = sample.split(data1$meal.served, SplitRatio = 0.8)
train = subset(data1, sp=="TRUE")
train
test = subset(data1, sp=="FALSE")
test
set.seed(120)
#fitting model
classifier1 = naiveBayes(meal.served~., data= train)
classifier1
#predicting
y_pred = predict(classifier1, newdata=test)
#confusion matrix
cmm = table(test$meal.served, y_pred)
cmm



5555555 5555555555555555555                           #Pratical- 5 DBSCAN                        55555555555555555555555555555555

data("iris")
str(iris)
#storing last variable
new = iris[,-5]
new
#packages
library(fpc)
library(dbscan)
#knn
kNNdistplot(new, k=3)
abline(h=0.5)
#clustering
set.seed(123)
f = fpc::dbscan(new, eps = 0.5, MinPts = 4)
f
#or
d = dbscan::dbscan(new,0.5,4)
d
#plotting
library(factoextra)
fviz_cluster(f,new, geom = "point")



6666666666666666666666666                            #practical-6 HIERARCHIAL clustering                                   66666666666666666666666666

df = read.csv("agespend.csv", header = TRUE)
df
df$age
#packages
library(factoextra)
library(cluster)
#distance of age
res.dist = dist(df$age, method = "euclidean")
res.dist
#matrix format
as.matrix(res.dist)
#hierarchical cluster and dendrogram
res.hc = hclust(res.dist, method = "complete")
res.hc
plot(res.hc)
#clustering
kcl = cutree(res.hc, k=4)
kcl
table(kcl)
#adding colors
fviz_dend(res.hc, k=4, cex = 0.8, k_colors = c("#1B9E77", "#D95F02", "#7570B3", "#E7298A"),
          color_labels_by_k = TRUE, rect = TRUE)
#forming cluster wrt height
height.cl = cutree(res.hc, h=20)
height.cl
table(height.cl)
fviz_dend(res.hc, k=2, cex = 0.8, k_colors = c( "#7570B3", "#E7298A"),
          color_labels_by_k = TRUE, rect = TRUE)



7777777777777777777777777777                      #Practical-7                   777777777777777777777777777777777777777777

#########################                      assesing cluster tendency                               ###########################

df = data("iris")
df
library(clustertend)
head(iris, 3)
df = iris[,-5]
df
#random dataset
random_df = apply(df,2, function(x){runif(length(x),min(x),max(x))})
random_df
#standardizing data
random_df = scale(random_df)
random_df
#plotting actual df
fviz_pca_ind(prcomp(df), title="PCA of iris", habillage = iris$Species,
             palette = "jco",geom = "point", ggtheme= theme_classic(), legend="bottom")
#plottin random df
fviz_pca_ind(prcomp(random_df), title="PCA of iris", habillage = iris$Species,
             palette = "jco",geom = "point", ggtheme= theme_classic(), legend="bottom")
#applying kmeans for acutal df
km.res = kmeans(df,3)
fviz_cluster(list(data=df, cluster=km.res$cluster),ellipse.type = "norm",
             geom="point", stand = FALSE, palette="jco",ggtheme = theme_classic())
#applying kmeans for random df
km.res = kmeans(random_df,3)
fviz_cluster(list(data=random_df, cluster=km.res$cluster),ellipse.type = "norm",
             geom="point", stand = FALSE, palette="jco",ggtheme = theme_classic())
#applying hopkins stats for actual df
library(hopkins)
hopkins(df, nrow(df)-1)

hopkins(random_df,nrow(random_df)-1)

########################                              assesing cluster library                       #########################################

data("USArrests")
head(USArrests,5)
#scaling
df = scale(USArrests)
df
#applying various methods to find optimal no of clusters
#1. elbow method
fviz_nbclust(df,kmeans,method = "wss")
#2. silhoutte method
fviz_nbclust(df,kmeans, method = "silhouette",nboot = 50)
#3. gap stat method
fviz_nbclust(df,kmeans, nstart=25, method = "gap_stat",nboot = 50)
#4. kmeans
library(NbClust)
nb =NbClust(df, distance = "euclidean",min.nc = 2,max.nc = 10, method = "kmeans")


 
88888888888888888888                              Practical - 8 ASSOCIATION RULESS                         888888888888888888888888888888888888

library(Matrix)
library(arules)
library(RColorBrewer)
data("Groceries")
View(Groceries@itemInfo)
rules = apriori(Groceries)
rules = apriori(Groceries, parameter = list(supp=0.01, conf= 0.2))
print(rules)
inspect(rules[221:232])
#visulalizing
arules::itemFrequencyPlot(Groceries, topN= 20, col = brewer.pal(8, 'Pastel2'),
                          main='Relative Item Frequency Plot', type = 'relative',
                          ylab='Item frequency (Relative)')
#association using eclat
frequentItems = eclat(Groceries, parameter = list(supp=0.07, maxlen=15))
print(frequentItems)
inspect(frequentItems)
#performing analysis
#sorting by confidence
rules_conf = sort(rules, by="confidence",decreasing = TRUE)
inspect(rules_conf)
#sorting by lift
rules_lift = sort(rules, by='lift', decreasing = TRUE)
inspect(rules_lift)
#viewing items bought along milk
rules = apriori(data = Groceries, parameter= list(supp=0.01, conf=0.08),
                appearance = list(default='lhs',rhs='whole milk'),control=list(verbose=F))
inspect(rules[0:10])



99999999999999999                               #practical - 9  SPATIAL data analysis                                            999999999999999999999999999

library(tidyverse)
library(sp)
library(sf)
library(mapview)
fishdata = read_csv("fishdat.csv")
str(fishdata)
statloc = read_csv("statloc.csv")
str(statloc)
sgdat = st_read("sgdat.shp")
head(sgdat)
methods(class = "sf")
#performing leftjoin
alldata = left_join(fishdata,statloc, by="Reference")
str(alldata)
#plotting
alldata = st_as_sf(alldata, coords = c("Longitude","Latitude"),crs = 4326)
plot(alldata, max.plot= 12)
#filtering data wrt particular year
filt_data = alldata%>%filter(yr==2016)
filt_data
#plotting
plot(filt_data$geometry)
sgdata = st_read("sgdat.shx")
#transforming crs to other value
alldata = alldata%>%st_transform(crs = "WGS84")
plot(alldata$geometry)
#cordinates
alldata = st_set_crs(alldata, 4326)
sgdata = st_set_crs(sgdata, 4326)
#visulalization
mapview(sgdata, col.regions = 'green')+ mapview(alldata, zcol='Gear')


10 10 10 10 10 10 10 10                       #Practical-10 NEURAL networks                  10 10 10 10 10 10 10 10 10 10

data = read.csv("cereals.csv", header = TRUE)
nrow(data)
#making sample size for training
samplesize = 0.60 * nrow(data)
samplesize
#picking 45 sample records
index = sample(seq_len(nrow(data)), size = samplesize)
index

datatrain = data[index,]
datatest = data[-index,]
max = apply(data, 2, max)
max
min = apply(data, 2, min)
min
#scaling data
scaled = as.data.frame(scale(data, center = min, scale=max-min))
scaled
library(neuralnet)
#training
traiNN = scaled[index,]
traiNN
testNN = scaled[-index,]
testNN
#fitting neural network
NN = neuralnet(rating ~ calories+protein+fat+sodium+fiber,
               traiNN, hidden = 3, linear.output = T)
plot(NN)
#predicting using nn
predict_testNN = compute(NN, testNN[,c(1:5)])
predict_testNN = (predict_testNN$net.result * (max(data$rating)-
                                                 min(data$rating)))+min(data$rating)
predict_testNN
#plotting
plot(datatest$rating, predict_testNN, col='blue',pch=16,
     ylab='Predicted rating NN', xlab="Real rating")
abline(0,1)
#calculation rmse
RMSE.NN = (sum((datatest$rating - predict_testNN)^2)/nrow(datatest))^0.5
RMSE.NN
                                      
